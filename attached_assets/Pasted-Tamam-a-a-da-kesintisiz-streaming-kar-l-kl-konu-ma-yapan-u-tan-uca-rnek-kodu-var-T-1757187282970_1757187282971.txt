Tamam â€” aÅŸaÄŸÄ±da **kesintisiz (streaming) karÅŸÄ±lÄ±klÄ± konuÅŸma** yapan uÃ§tan uca Ã¶rnek kodu var:

* TarayÄ±cÄ± â†’ WebSocket â†’ Node.js sunucu â†’ **Azure Speech (continuous STT)** â†’ **Gemini (LLM)** â†’ **ElevenLabs (TTS)** â†’ ses tarayÄ±cÄ±da Ã§alÄ±nÄ±r â†’ ardÄ±ndan sunucu tekrar dinlemeye geÃ§er.
* Yani: kullanÄ±cÄ± konuÅŸur â†’ asistan cevap verir â†’ tekrar dinler â†’ dÃ¶ngÃ¼ devam eder.

Bu PoC production-grade hata/Ã¶lÃ§ek/kimlik doÄŸrulama kontrolleri iÃ§ermez fakat gerÃ§ek kullanÄ±m iÃ§in doÄŸrudan Ã§alÄ±ÅŸacak ÅŸekilde tasarlandÄ±.

---

## Gereksinimler / .env

Node projesi kurulu olsun:

```bash
npm init -y
npm i express ws microsoft-cognitiveservices-speech-sdk node-fetch dotenv @google/generative-ai
```

`.env` dosyasÄ±:

```
PORT=8080
AZURE_SPEECH_KEY=YOUR_AZURE_KEY
AZURE_SPEECH_REGION=YOUR_REGION   # Ã¶rn: westeurope
GEMINI_API_KEY=YOUR_GEMINI_KEY
GEMINI_MODEL=gemini-1.5-pro
ELEVENLABS_API_KEY=YOUR_ELEVENLABS_KEY
ELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM
```

---

## `server.js` (Node.js)

```js
// server.js
require("dotenv").config();
const fs = require("fs");
const http = require("http");
const express = require("express");
const WebSocket = require("ws");
const sdk = require("microsoft-cognitiveservices-speech-sdk");
const fetch = require("node-fetch");
const { GoogleGenerativeAI } = require("@google/generative-ai");

const app = express();
app.use(express.static("public"));
const server = http.createServer(app);
const wss = new WebSocket.Server({ server });

const PORT = process.env.PORT || 8080;

// Gemini init (assumes @google/generative-ai usage as in earlier examples)
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: process.env.GEMINI_MODEL || "gemini-1.5-pro" });

// Helper: Gemini (sync) response
async function generateReplyWithGemini(userText, conversationHistory = []) {
  // conversationHistory = [{role:"user"|"assistant", text: "..."}]
  const promptParts = [
    { role: "system", content: { text: "Sen TÃ¼rkÃ§e, kÄ±sa ve doÄŸal cevaplar veren sesli asistansÄ±n." } },
    ...conversationHistory.flatMap(m => [{ role: m.role, content: { text: m.text } }]),
    { role: "user", content: { text: userText } }
  ];
  // Using generateContent path similar to earlier snippet:
  const resp = await model.generateContent({ contents: [{ role: "user", parts: [{ text: userText }] }] });
  // Fallback extracting text:
  const out = resp.response?.text?.() ?? resp.response?.candidates?.[0]?.content?.parts?.[0]?.text ?? "";
  return out.trim();
}

// Helper: ElevenLabs TTS -> returns MP3 Buffer
async function elevenlabsTTS(text) {
  const url = `https://api.elevenlabs.io/v1/text-to-speech/${process.env.ELEVENLABS_VOICE_ID}`;
  const body = {
    text,
    model_id: "eleven_multilingual_v2",
    voice_settings: { stability: 0.4, similarity_boost: 0.7 }
  };
  const res = await fetch(url, {
    method: "POST",
    headers: {
      "xi-api-key": process.env.ELEVENLABS_API_KEY,
      "Content-Type": "application/json",
      "Accept": "audio/mpeg"
    },
    body: JSON.stringify(body)
  });
  if (!res.ok) {
    const txt = await res.text();
    throw new Error(`ElevenLabs TTS failed: ${res.status} ${txt}`);
  }
  return await res.arrayBuffer();
}

// Per-connection handler
wss.on("connection", (ws, req) => {
  console.log("ğŸŸ¢ Client connected");

  // Create Azure push stream and recognizer
  const pushStream = sdk.AudioInputStream.createPushStream(sdk.AudioStreamFormat.getWaveFormatPCM(16000, 16, 1));
  const speechConfig = sdk.SpeechConfig.fromSubscription(process.env.AZURE_SPEECH_KEY, process.env.AZURE_SPEECH_REGION);
  speechConfig.speechRecognitionLanguage = "tr-TR";
  let audioConfig = sdk.AudioConfig.fromStreamInput(pushStream);
  let recognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);

  // Conversation history (very small buffer to keep context)
  const convoHistory = [];

  let isProcessing = false; // cevabÄ± Ã¼retirken yeni final event'leri kÄ±sa devre et

  function startRecognitionHandlers() {
    recognizer.recognizing = (s, e) => {
      const partial = e.result?.text ?? "";
      if (partial) ws.send(JSON.stringify({ type: "partial", text: partial }));
    };

    recognizer.recognized = async (s, e) => {
      if (!e.result) return;
      const text = e.result.text || "";
      const reason = e.result.reason;
      // EÄŸer boÅŸsa atla
      if (!text || text.trim().length === 0) return;

      // Ignore if we are already processing a previous final
      if (isProcessing) return;

      // Final result arrives
      isProcessing = true;
      ws.send(JSON.stringify({ type: "final", text }));

      try {
        // Add user utterance to history
        convoHistory.push({ role: "user", text });

        // Generate reply
        const reply = await generateReplyWithGemini(text, convoHistory);
        convoHistory.push({ role: "assistant", text: reply });

        ws.send(JSON.stringify({ type: "reply", text: reply }));

        // TTS (ElevenLabs)
        const audioBuf = await elevenlabsTTS(reply); // ArrayBuffer
        const base64 = Buffer.from(audioBuf).toString("base64");
        // Send tts chunk (mp3)
        ws.send(JSON.stringify({ type: "tts", format: "mp3", base64 }));

        // small pause to ensure client plays audio, then resume recognition
        setTimeout(() => {
          isProcessing = false;
        }, 300); // 300ms pause; adjust as needed
      } catch (err) {
        console.error("Pipeline error:", err);
        ws.send(JSON.stringify({ type: "error", error: String(err) }));
        isProcessing = false;
      }
    };

    recognizer.canceled = (s, e) => {
      console.warn("Azure canceled:", e);
      ws.send(JSON.stringify({ type: "error", error: "Azure canceled: " + (e.errorDetails || e.reason) }));
    };

    recognizer.sessionStopped = () => {
      console.log("Azure session stopped");
    };
  }

  startRecognitionHandlers();
  recognizer.startContinuousRecognitionAsync();

  ws.on("message", (msg) => {
    // We expect binary PCM16 chunks from browser
    if (typeof msg === "string") {
      // control messages can be JSON
      try {
        const data = JSON.parse(msg);
        if (data && data.type === "control" && data.action === "end") {
          // optionally close pushStream if client indicates end
          try { pushStream.close(); } catch {}
        }
      } catch (e) {
        // not JSON
      }
      return;
    }

    // binary -> pushStream.write
    if (Buffer.isBuffer(msg) || msg instanceof ArrayBuffer) {
      try {
        // Convert to Node Buffer and write
        const buf = Buffer.isBuffer(msg) ? msg : Buffer.from(msg);
        pushStream.write(buf.slice()); // write chunk in PCM16LE 16kHz
      } catch (err) {
        console.error("pushStream write error:", err);
      }
    }
  });

  ws.on("close", () => {
    console.log("Client disconnected");
    try { recognizer.stopContinuousRecognitionAsync(() => recognizer.close(), () => {}); } catch {}
    try { pushStream.close(); } catch {}
  });

  ws.on("error", (err) => {
    console.error("WS error", err);
  });
});

server.listen(PORT, () => {
  console.log(`Server listening at http://localhost:${PORT}`);
});
```

---

## `public/index.html` (istemci â€” continuous streaming + otomatik tekrar dinleme)

```html
<!DOCTYPE html>
<html lang="tr">
<head>
  <meta charset="utf-8" />
  <title>Streaming KarÅŸÄ±lÄ±klÄ± Sesli Asistan</title>
  <style>
    body{font-family:system-ui,Segoe UI,Roboto,Arial;margin:20px}
    button{padding:.6rem 1rem;margin-right:.5rem}
    #status{margin-top:10px;font-weight:bold}
    #transcript,#reply{border:1px solid #ddd;padding:10px;min-height:40px;margin-top:8px}
  </style>
</head>
<body>
  <h2>Streaming KarÅŸÄ±lÄ±klÄ± Asistan</h2>
  <button id="start">BaÅŸlat</button>
  <button id="stop" disabled>Durdur</button>
  <div id="status">HazÄ±r</div>

  <h3>KullanÄ±cÄ±</h3>
  <div id="transcript"></div>

  <h3>Asistan</h3>
  <div id="reply"></div>

  <audio id="player" controls></audio>

<script>
const startBtn = document.getElementById("start");
const stopBtn = document.getElementById("stop");
const statusEl = document.getElementById("status");
const transcriptEl = document.getElementById("transcript");
const replyEl = document.getElementById("reply");
const player = document.getElementById("player");

let ws, audioCtx, mediaStream, sourceNode, processor;

function logStatus(s){ statusEl.textContent = s; }

function floatTo16BitPCM(float32Array){
  const buffer = new ArrayBuffer(float32Array.length * 2);
  const view = new DataView(buffer);
  let offset = 0;
  for (let i = 0; i < float32Array.length; i++, offset += 2) {
    let s = Math.max(-1, Math.min(1, float32Array[i]));
    view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
  }
  return new Uint8Array(buffer);
}

function downsampleBuffer(buffer, inSampleRate, outSampleRate = 16000) {
  if (outSampleRate === inSampleRate) return buffer;
  const sampleRateRatio = inSampleRate / outSampleRate;
  const newLength = Math.round(buffer.length / sampleRateRatio);
  const result = new Float32Array(newLength);
  let offsetResult = 0, offsetBuffer = 0;
  while (offsetResult < result.length) {
    const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);
    let accum = 0, count = 0;
    for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
      accum += buffer[i];
      count++;
    }
    result[offsetResult] = accum / count;
    offsetResult++;
    offsetBuffer = nextOffsetBuffer;
  }
  return result;
}

// We will send chunk sizes that are multiples of 3200 bytes (1600 samples * 2 bytes)
function sendPCMChunk(float32Array) {
  const down = downsampleBuffer(float32Array, audioCtx.sampleRate, 16000);
  const pcm16 = floatTo16BitPCM(down);
  if (ws && ws.readyState === WebSocket.OPEN) {
    // Optionally slice into smaller frames (e.g., 3200 bytes)
    const maxChunk = 3200; // bytes
    for (let i = 0; i < pcm16.length; i += maxChunk) {
      const chunk = pcm16.slice(i, i + maxChunk);
      ws.send(chunk);
    }
  }
}

async function start() {
  startBtn.disabled = true;
  stopBtn.disabled = false;
  transcriptEl.textContent = "";
  replyEl.textContent = "";
  player.src = "";
  logStatus("BaÄŸlanÄ±yor...");

  ws = new WebSocket(`ws://${location.host}`);
  ws.binaryType = "arraybuffer";

  ws.onopen = async () => {
    logStatus("WS aÃ§Ä±k, mikrofon aÃ§Ä±lÄ±yor...");
    mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 });
    sourceNode = audioCtx.createMediaStreamSource(mediaStream);

    // ScriptProcessor is deprecated but safe PoC; alternatives: AudioWorklet
    processor = audioCtx.createScriptProcessor(4096, 1, 1);
    processor.onaudioprocess = (e) => {
      const input = e.inputBuffer.getChannelData(0);
      sendPCMChunk(input);
    };

    sourceNode.connect(processor);
    processor.connect(audioCtx.destination);

    logStatus("Dinleniyor...");
  };

  ws.onmessage = (ev) => {
    try {
      const data = JSON.parse(ev.data);
      if (data.type === "partial") {
        transcriptEl.textContent = "â³ " + data.text;
      } else if (data.type === "final") {
        transcriptEl.textContent = "ğŸ—£ï¸ " + data.text;
        logStatus("Cevap hazÄ±rlanÄ±yor...");
      } else if (data.type === "reply") {
        replyEl.textContent = "ğŸ’¡ " + data.text;
      } else if (data.type === "tts") {
        const src = `data:audio/${data.format};base64,${data.base64}`;
        player.src = src;
        player.play().catch(e => console.warn("play error", e));
        // After audio starts playing, status -> tekrar dinleyecek
        player.onended = () => logStatus("Dinleniyor...");
      } else if (data.type === "error") {
        logStatus("Hata: " + data.error);
      }
    } catch(e){
      // not JSON
    }
  };

  ws.onclose = () => {
    logStatus("WS kapandÄ±.");
    cleanup();
  };
  ws.onerror = (err) => {
    console.error("WS error", err);
    logStatus("WS hata");
    cleanup();
  };
}

function stop() {
  if (ws && ws.readyState === WebSocket.OPEN) {
    ws.send(JSON.stringify({ type: "control", action: "end" }));
    ws.close();
  }
  cleanup();
}

function cleanup() {
  startBtn.disabled = false;
  stopBtn.disabled = true;
  try { processor && processor.disconnect(); } catch {}
  try { sourceNode && sourceNode.disconnect(); } catch {}
  try { audioCtx && audioCtx.close(); } catch {}
  try { mediaStream && mediaStream.getTracks().forEach(t => t.stop()); } catch {}
  logStatus("Durduruldu");
}

startBtn.onclick = start;
stopBtn.onclick = stop;
</script>
</body>
</html>
```

---

## Ã–nemli notlar â€” neden iÅŸe yarar / olasÄ± hatalar ve Ã§Ã¶zÃ¼mleri

1. **Ses formatÄ± kritik**: TarayÄ±cÄ±dan gÃ¶nderdiÄŸimiz chunkâ€™lar **PCM16LE, 16 kHz, mono** olmalÄ±. Ä°stemci kodunda bunu saÄŸladÄ±k (downsample + int16 convert). Azureâ€™a giden pushStream de WAV formatÄ± beklentisine uygun `AudioInputStream` ile yazÄ±lÄ±yor.

2. **Chunk boyutu**: LoglarÄ±nda `2730` gibi dÃ¼zensiz uzunluk gÃ¶rmÃ¼ÅŸtÃ¼n â€” Azure bazen tam frame boundary bekleyebilir. Ä°stemcide 3200 byteâ€™lÄ±k adÄ±mlarla gÃ¶nderme mantÄ±ÄŸÄ± uyguladÄ±m (1600 sample \* 2 byte). Bu sayede daha stabil sonuÃ§ alÄ±rsÄ±n.

3. **Dil ayarÄ±**: `speechConfig.speechRecognitionLanguage = "tr-TR"` olarak ayarlandÄ± â€” TÃ¼rkÃ§e ise gerekli.

4. **Concurrency / isProcessing flag**: Sunucu, final event geldiÄŸinde `isProcessing = true` yapÄ±p yeni final'leri cevap Ã¼retimi sÃ¼rerken yoksayar. Bu dÃ¶ngÃ¼nÃ¼n kararlÄ±lÄ±ÄŸÄ±nÄ± artÄ±rÄ±r.

5. **UE / Play izinleri**: TarayÄ±cÄ± otomatik Ã§alma kÄ±sÄ±tlamalarÄ± olabilir â€” kullanÄ±cÄ± ilk etkileÅŸim ile `BaÅŸlat` butonuna tÄ±klamalÄ± (bunu saÄŸladÄ±k).

6. **Latency tuning**: KÃ¼Ã§Ã¼k beklemeler, chunk boyutlarÄ± ve ElevenLabs dÃ¶nÃ¼ÅŸ sÃ¼releri gerÃ§ek deneyde ayarlanmalÄ±.

---

EÄŸer istersen ÅŸimdiye kadar:

* Bu kodu senin iÃ§in tek bir GitHub Gist / zip haline getireyim (indirmeye hazÄ±r), **veya**
* Azure tarafÄ±nda hata alÄ±yorsan senin Ã¶nceki loglardaki spesifik `52494646...` headerâ€™lÄ± Ã¶rnekleri kullanarak debug yapalÄ±m â€” bana bir Ã¶rnek WAV/PCM dosyasÄ± Base64 ya da hatalÄ± chunk boyutlarÄ±nÄ± gÃ¶ster (ama Ã¶zel gizli anahtar **gÃ¶nderme**), ben potansiyel byte-alignment hatalarÄ±nÄ± gÃ¶stereyim.

Hangi adÄ±mÄ± istersin? (Ä°ndirilebilir zip mi, yoksa baÄŸlanÄ±p debug mu?)
